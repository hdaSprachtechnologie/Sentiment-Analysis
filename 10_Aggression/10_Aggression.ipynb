{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 10: Erkennung und Klassifikation von Aggression in Meinungsäußerungen\n",
    "\n",
    "Mit dem Aufkommen der sozialen Netzwerke entstand eine Problematik, die sich in letzter Zeit deutlich verstärkt hat: Aggressive, hasserfüllte, beleidigende Postings, bis hin zu Bedrohungen von Politikern, Journalistinnen und anderen Menschen, die ihre Meinung äußern. Es bleibt nicht immer bei verbaler Aggression, in manchen Fällen folgten darauf Verbrechen, wie 2019 im Fall des Kasseler Regierungspräsidenten Walter Lübcke (https://de.wikipedia.org/wiki/Mordfall_Walter_L\\%C3\\%BCbcke). \n",
    "\n",
    "Aggression in Meinungsäußerungen umfasst das weite Spektrum vom Gebrauch von Schimpfwörtern über Beleidigungen und Diskriminierungen bis hin zu Gewaltandrohungen (siehe Ruppenhofer et al 2018).  Die sozialen Medien wie Twitter, Facebook und auch die Kommentarspalten der Online-Präsenzen von Zeitungen und Radiosendern werden zunehmend von Menschen dominiert, die diffamieren, beleidigen und bedrohen. In einer Studie der Landesanstalt für Medien NRW wurde festgestellt: \n",
    "\n",
    "    So gibt die überwiegende Mehrheit der Befragten (78 %) an, schon einmal Hassrede bzw. Hasskommentare im Internet gesehen zu haben, z. B. auf Webseiten, in Blogs, in sozialen Netzwerken oder in Internetforen. Davon geben 10 Prozent an, schon sehr häufig Hassrede bzw. Hasskommentare im Internet gesehen, 26 Prozent häufig und 42 Prozent weniger häufig.\n",
    "\n",
    "\n",
    "Automatisch generierte Nachrichten werden verwendet, um den Eindruck zu erwecken, dass diese extremen Meinungen in der Bevölkerung weit verbreitet sind, aber auch, um politische Gegner mundtot zu machen. Infolgedessen gelingt es vielen Betreibern von Social-Media-Webseiten nicht mehr, Nutzerbeiträge manuell zu moderieren, und die Moderation hasserfüllter Kommentarspalten bedeutet für die Moderatoren eine enorme psychische Belastung. Betreiber von sozialen Netzwerken werden verpflichtet, strafbare Inhalte nicht nur zu filtern sondern auch zu melden.  Daher besteht ein dringender Bedarf an Methoden zur automatischen Identifizierung verdächtiger Beiträge. \n",
    "\n",
    "Im ersten Schritt ist es aber notwendig zu definieren, was eigentlich als verdächtige Beiträge klassifiziert werden soll. Im Rahmen des GermEval Projekts 2018 und 2019 (https://projects.fzai.h-da.de/iggsa/) wurde ein Dokument mit \"Annotation Guidelines\" erstellt, das sehr spezifische Richtlinien für diese Klassifikation enthält.\n",
    "\n",
    "(Notiz: Es lässt sich nicht ganz vermeiden, dass wir einige sehr hässliche Beispiele hier geben. Diese spiegeln natürlich nicht unsere Meinung wider.)\n",
    "\n",
    "Die Hauptkategorien sind:\n",
    "- INSULT: Zuschreibung von negativ bewerteten Qualitäten oder Mängeln oder die Kennzeichnung von Personen als unwürdig oder nicht wertvoll. Beleidigungen vermitteln Respektlosigkeit und Verachtung.\n",
    "    - z. B.: ein \\#Tatort mit der Presswurst \\#Saalfeld geht gar nicht \\#ARD\n",
    "- ABUSE: Das Ziel der Bewertung wird als Repräsentant einer Gruppe angesehen. Dieser Gruppe werden negative Eigenschaften zugewiesen, die universal, omnipräsent und unveränderbar sind. Weitere Formen sind die Entmenschlichung eines Individuums und Bedrohungen.\n",
    "    - z. B.: Ich persönlich scheisse auf die grüne Kinderfickerpartei\n",
    "- PROFANITY: Nicht akzeptierbare Sprache kann auch ohne Beleidigung oder Diskriminierung auftreten. Es handelt sich hierbei um Schimpfwörter und Flüche. \n",
    "    - z. B.: ob ich sterbe darauf geb ich fick\n",
    "- OTHER: Alle Äußerungen, die positiv oder neutral sind oder negative Äußerungen, die nicht unter die Kategorien INSULT, ABUSE oder PROFANITY fallen. Ironische beleidigende Äußerungen, die oberflächlich betrachtet positiv sind, fallen nicht unter diese Kategorie. \n",
    "    - z. B.: Hamas-Vertreter bekräftigt Ziel der Zerstörung Israels\n",
    "\n",
    "Ein Hauptmerkmal von ABUSE ist, dass die dem Ziel zugeschriebenen negativen Qualitäten direkt aus seiner Zugehörigkeit zu einer Gruppe oder einem Kollektiv folgen, von den anderen Mitgliedern der Gruppe geteilt werden und unveränderlich sind. Die prototypischen Klassen sind also diejenigen, die durch die Geburt definiert sind, wie Geschlecht, Nationalität, Ethnizität, Glaube. Es gibt jedoch auch andere Klassen, die, obwohl sie nicht durch die Geburt definiert sind, Ziele von Diskriminierung sein können, zum Beispiel Migranten/Flüchtlinge oder politische Parteien.\n",
    "\n",
    "Gruppen, die häufig Ziele von ABUSE sind:\n",
    "- Feministinnen\n",
    "-  Menschen mit schwarzer Hautfarbe\n",
    "-  Muslime\n",
    "-  Juden\n",
    "-  Homosexuelle (LGBT)\n",
    "-  Flüchtlinge\n",
    "-  Mitglieder politischer Parteien\n",
    "-  Journalisten\n",
    "\n",
    "Den Gruppen werden häufig vermeintliche Eigenschaften - Stereotype wie Faulheit, Unsauberkeit, sexuelle Perversion - zugeschrieben. Personen oder Gruppen von Personen werden verflucht (\"Die soll der Teufel holen!\") oder bedroht (\"Den mach ich fertig!\"). In einigen Fällen wird zur Gewalt aufgerufen (\"Haut den ... in die Fresse!\"). \n",
    "\n",
    "\n",
    "Es bleibt - trotz umfassender Definitionen - letztlich unklar, welche Merkmale ABUSE-Sprache im Detail notwendigerweise haben muss, um als solche klassifiziert zu werden. \n",
    "Muss eine Person oder Personengruppe direkt angesprochen werden oder reicht es, wenn indirekte Formen der Bezugnahme auf Personen und Gruppen vorliegen (z. B. eine Erwähnung von \"Negermusik\" im Bezug auf eine Fernsehsendung)? Diese Fragen lassen sich nicht objektiv und aus rein sprachlicher Analyse entscheiden. Welche Art der sprachlichen Äußerung als Herabsetzung gilt, welche Zuschreibung zu einer marginalisierten sozialen Identität führt ist, muss letztlich gesellschaftlich ausgehandelt werden. Eine Software lernt aus den Klassifikationen, die von Menschen vorgegeben wurden. Daher ist es erstens wichtig, dass die zugrundeliegenden annotierten Daten sorgfältig erstellt wurden und zweitens, dass das Ergebnis der Software immer auch durch Menschen evaluiert wird, dass also nicht ein automatischer Filter sondern nur ein Warnsystem implementiert wird.\n",
    "\n",
    "## Daten, Daten, Daten\n",
    " \n",
    " \n",
    " Wie wir schon gesehen haben, ist eine wichtige Voraussetzung für die automatische Klassifikation von Texten die Verfügbarkeit von annotierten Daten. \n",
    "Um die Forschung und Entwicklung in einem Themengebiet voranzubringen, wird im Bereich der Sprachverarbeitung daher häufig zunächst ein Wettbewerb aufgesetzt, eine sogenannte \"Shared Task\". Wir haben ja bereits mit den Daten der GermEval Shared Task 2017 zu Kundenkommentaren der deutschen Bahn gearbeitet. Für die automatische Klassifikation aggressiver Meinungsäußerungen gab es in den letzten Jahren einige Shared Tasks:\n",
    "- Kaggle’s 2018 Toxic Comment Classification Challenge (https://www.kaggle.com/c/jigsawtoxic-comment-classification-challenge) war eine Shared Task, die sich mit Kommentaren im englischsprachigen Wikipedia beschäftigte. Es gab sechs Kategorien für die Klassifikation: \"toxic\", \"severe toxic\", \"obscene\", \"insult\", \"identity hate\", \"threat\".\n",
    "- Bei der TRAC Shared Task on Aggression Identification (Kumar et al 2018) ging es um Facebook-Kommentare in Englisch und Hindi. Die teilnehmenden Forschungsgruppen mussten diskriminierende Kommentare entdecken und offene und versteckte Aggression klassifizieren. \n",
    "- Die Shared Task on Automatic Misogyny Identification (AMI) (Fersini et al 2018) betraf englische, spanische und italienische Tweets. Der Fokus lag hier auf Tweets mit sexistischem Inhalt. Neben der Entdeckung sexistischer Tweets wurden diese auch klassifiziert. Die Klassen waren \"Discredit\", \"Derailing\", \"Dominance\", \"Sexual Harassment & Threats of Violence\", \"Stereotype & Objectification\", \"Active\", \"Passive\"}.\n",
    "- Bei SemEval 2019 - Task 5 (HatEval)  (Basile et al 2019) ging es um die automatische Erkennung von englisch- und spanischsprachiger Hassrede gegen Immigranten auf Twitter. In zwei Subtasks wurden die Tweets einmal als Hate Speech oder nicht klassifiziert und einmal wurde das Ziel des Angriffs als individuell oder generisch klassifiziert, was grob der Unterscheidung zwischen \"Insult\" und \"Abuse\" entspricht. Bei SemEval 2019 - Task 6 (OffensEval 2019) (Zampieri et al 2019) ging es um  englischsprachige Tweets. Das Datenset bestand aus 14.000 Tweets. Aus diesen sollten die offensiven Tweets extrahiert und klassifiziert werden. Außerdem sollte erkannt werden, wer oder was dort angegriffen wurde. Auch 2020 wurde die OffensEval als SemEval 2020 - Task 12 fortgeführt. Hier ging es um ein mehrsprachiges Datenset mit arabischen, dänischen, englischen, griechischen und türkischen Daten (https://sites.google.com/site/offensevalsharedtask/). \n",
    "- GermEval 2018 (Wiegand et al 2018) beschäftigte sich mit der Klassifikation deutschsprachiger Twitterdaten als \"Insult\", \"Abuse\", \"Profanity\" oder eben nicht offensiv, \"Other\". In einer zweiten Auflage, bei GermEval 2019 (Struss et al 2019) wurden zusätzlich offensiveTweets als explizit und implizit klassifiziert. Insgesamt besteht das Datenset aus über 15.000 annotierten deutschsprachigen Tweets. Diese Daten sind auf der Projektwebseite (https://projects.fzai.h-da.de/iggsa/data-2019) verfügbar. Wir können sie nutzen, um ein Klassifikationssystem für Tweets zu implementieren.\n",
    "\n",
    "Neben einer möglichst präzisen Definition dessen, was klassifiziert werden soll, ist eine umfassende Analyse zur Art der Daten sehr wichtig. Man könnte z. B. hingehen und alle Tweets der letzten zwei Wochen aus Twitter extrahieren und dann annotieren. Das Ergebnis wäre ein Daten-Set mit sehr wenigen offensiven Tweets, wahrscheinlich nur 2-3% . Eine Klassifikation, die dann jeden Tweet als nicht offensiv klassifizieren würde, hätte sehr gute Gesamtwerte in der Evaluation, denn sie würde ja in den meisten Fällen richtig liegen (siehe auch unsere Diskussion des Accuracy-Werts bei unausgewogenen Daten im Kapitel \"Qualitätssicherung und systematische Evaluation). Diese Klassifikation würde uns aber nicht weiterhelfen bei der Lösung des Problems. Man könnte auch mit offensiven Stichwörtern - also z. B. Schimpfwörtern - nach offensiven Tweets suchen. Eine Klassifikation könnte dann aber einfach einen Abgleich mit diesen Stichwörtern machen und wäre dann schnell fertig. Tweets mit Wörtern, die nicht in dieser Liste stehen, würden aber nicht erkannt.\n",
    "Bei der GermEval 2018 und 2019 haben wir daher einen anderen Weg gewählt: Wir haben zunächst mit einschlägigen Stichwörtern nach Twitter-Usern gesucht, die häufig auch offensive Tweets posten. Dann haben wir aus den Timelines dieser User 200 Tweets ausgewählt und diese dann annotiert. Die Information über die User haben wir dann wieder gelöscht und die Daten um Retweets, extrem kurze Tweets und Tweets mit Links bereinigt.\n",
    "Auf diese Weise bekamen wir offensive und nicht offensive Tweets, die sich um ähnliche Themenfelder drehen und die sich sprachlich nicht allzu sehr unterscheiden, wie das der Fall wäre, wenn wir z. B. Tweets von Zeitungsredaktionen mit offensiven Tweets privater User gesammelt hätten.\n",
    "Da die Daten zu einem bestimmten Zeitpunkt gesammelt wurden, überwiegen Themen, die zu diesem Zeitpunkt aktuell in der Öffentlichkeit diskutiert wurden. Ein Problem ist noch, dass bestimmte Themen in diesen Daten überwiegend negativ bewertet wurden. Dies betraf z. B. Tweets zu Angela Merkel und Heiko Maas. Dies würde es einem Klassifikator wieder zu einfach machen.\n",
    " Zum sogenannten \"Debiasing\" der Daten haben wir daher noch neutrale oder positive Tweets zu diesen Themen aufgenommen. Im Fall der beiden Politiker waren das Tweets der Parteien CDU und SPD.\n",
    "\n",
    "Der nächste Schritt ist die Annotation. Hier ist besondere Sorgfalt angesagt, denn wir benötigen nicht nur viele annotierte Daten, sondern die Annotation muss möglichst konsistent sein. Daher haben wir zunächst einige Daten mit drei Annotatoren parallel annotiert und die Annotationen dann verglichen und diskutiert. Im Rahmen dieser Diskussionen sind die oben erwähnten Richtlinien zur Annotation entstanden. Manche Äußerungen sind sehr schwer zu klassifizieren. Was würden Sie z. B. mit diesen machen, wenn Sie die Möglichkeiten OTHER, ABUSE, INSULT und PROFANITY haben?\n",
    "\n",
    "- Meine Kollegin hat unendlich gute Laune , weil sie nächste Woche drei Wochen Urlaub hat . Wünsche ihr den Tod .\n",
    "- 24 x \\enquote{Digga} , 41 x \\enquote{Ich schwör} und 3 x \\enquote{Drecksjude} . Manche Imbissnachbarn möchte man gerne mal unbetäubt durchs Sieb streichen .\n",
    "- Eine Ergänzung zur Täterbeschreibung. Täterherkunft fehlt. \\#Rosenheimerplatz \\#Messerattacke \\#München\n",
    "- Alle 11 Minuten fällt ein liberales Backpfeifengesicht auf einen Spruch von Merkel rein.\n",
    "\n",
    "\n",
    "Unklare Fälle haben wir schließlich nicht annotiert und aus den Daten herausgenommen. \n",
    "\n",
    "Bei der Aufteilung der annotierten Daten in Trainings- und Testset haben wir dann noch darauf geachtet, dass die Klassenverteilung in den beiden Datensätzen sehr ähnlich ist. \n",
    "\n",
    "## Methoden zur automatischen Klassifikation\n",
    "\n",
    "Einen Überblick über aktuelle Methoden zur automatischen Erkennung von aggressiver Sprache gibt \\cite{Mishra+al:2019a}. Es wird dort deutlich, dass die weitaus meisten Methoden für englischsprachige Datensätze entwickelt wurden. Die Forschungsgruppen arbeiten mit ähnlichen Methoden wie die, die wir auch in der Sentiment-Analyse kennengelernt haben, mit Wörterbüchern, maschinellem Lernen und N-Grammen. \n",
    "\n",
    "Mit den Daten der GermEval 2019\\index{GermEval 2019} werden wir nun unser eigenes Klassifikationssystem implementieren. Dabei beginnen wir damit, ein heuristisches Verfahren mit Grenzwerten zu implementieren, das  unsere Sentiment-Analyse  mit Wortlisten kombiniert. Wir beschränken uns auf die binäre Klassifikation in OFFENSE - OTHER.\n",
    "Laden Sie zunächst die Trainingsdaten und die Testdaten der GermEval 2019 von der Website des Projekts herunter (https://projects.fzai.h-da.de/iggsa/data-2019/).\n",
    "\n",
    "Die annotierten Trainingsdaten haben ein sehr einfaches Format, z. B.:\n",
    "\n",
    "\n",
    "    @anna\\_IIna Kann man diesen ganzen Scheiß noch glauben..?\tOFFENSE\tPROFANITY\n",
    "\n",
    "\n",
    "Für die binäre Klassifikation ignorieren wir die rechts stehende Annotation\\index{Annotation} (hier: PROFANITY). Unser Klassifikationssystem soll den Testdaten, die keine Annotation haben,  eine Annotation hinzufügen.\n",
    "Die grundlegende Idee: Wir nutzen unsere Sentiment-Analyse, um positive und neutrale Aussagen mit OTHER zu klassifizieren. Bei negativen Aussagen gleichen wir diese mit zwei Wortlisten ab: einer Wortliste für ABUSE oder INSULT und einer Wortliste für PROFANITY.\n",
    "\n",
    "Im ersten Schritt müssen wir die Tweets vorverarbeiten, das sogenannte \"Preprocessing\". Wir löschen alle Zeichen heraus, die uns in der Programmierung Schwierigkeiten machen und schicken den Tweet durch spaCy für die Tokenisierung und eine linguistische Analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def preprocessing(sent):\n",
    "    sent = \"\".join((i if ord(i) < 10000 else '\\ufffd' for i in sent))\n",
    "    sent = sent.replace('|LBR|','')\n",
    "    analysis = nlp(sent)\n",
    "    return(sent, analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die binäre Klassifikation arbeiten wir mit separaten Wortlisten für OFFENSE (ABUSE, INSULT) und PROFANITY. Der Grund dafür ist: PROFANITY kann auch bei positivem Sentiment auftreten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wortlistenvergleich import sentiment_analysis\n",
    "from offensive_words import *\n",
    "from profane_words import *\n",
    "\n",
    "def hate_or_not(sent):\n",
    "    sent, analysis = preprocessing(sent)              # die Vorverarbeitung, die wir oben definiert haben\n",
    "    offense_value = 0\n",
    "    profane_value = 0\n",
    "    sentiment_value = sentiment_analysis(sent)               # hier die eigene Sentiment-Lösung aufrufen\n",
    "    if sentiment_value > -0.1:                         # wenn der Tweet positives oder neutrales Sentiment hat, dann kann er höchsten PROFANITY sein\n",
    "        offense_value = 0\n",
    "        for token in analysis:\n",
    "            if token.text.lower() in profane_words:     # dafür wird nach Wörtern in der Liste der profanen Wörter nachgesehen\n",
    "                profane_value = profane_value + 1\n",
    "    else:                                                                # ein Tweet mit negativem Sentiment \n",
    "        for token in analysis:\n",
    "            if token.text.lower() in offensive_words:  # es wird in der Liste mit offensiven Wörtern nachgesehen\n",
    "                offense_value = offense_value + 1\n",
    "            elif token.text.lower() in profane_words:  # außerdem in der Liste mit profanen Wörtern\n",
    "                profane_value = profane_value + 1\n",
    "    return (sentiment_value, offense_value, profane_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_or_not(\"Das ist völliger scheiß\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Algorithmus gibt zunächst nur Werte aus, keine Entscheidung. In einem zweiten Schritt müssen die Grenzwerte festgelegt werden: Ab welchen Werten für \"offense_value\" und \"profane_value\" soll entschieden werden, den Tweet als OFFENSE  zu klassifizieren? Diese Entscheidung sollte anhand von Tests auf den Trainingsdaten getroffen werden.\n",
    "Eine Alternative dazu, einen Grenzwert festzulegen, ist, die Werte in ein System zum maschinellen Lernen zu füttern.\n",
    "Dazu schreiben wir sie in die Trainingsdaten herein, um dann ein Modell darauf zu trainieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "out = open('training_annotated.txt','w', encoding='utf-8', errors='ignore')                       # wir öffnen die Ausgabedatei\n",
    "\n",
    "with open('germeval2019.training_subtask1_2.txt','r', encoding='utf-8', errors='ignore', newline='') as csvfile:               # wir öffnen die Trainingsdatei, die im TSV-Format vorliegt\n",
    "        training_vanilla = csv.reader(csvfile, delimiter='\\t')                           \n",
    "        for line in training_vanilla:\n",
    "            (sentiment_value, offense_value, profane_value) = hate_or_not(line[0])                        # wir berechnen die Werte für Sentiment, Offense und Profane\n",
    "            out.write(line[0] + '\\t' + str(sentiment_value)+ '\\t' + str(offense_value) + '\\t' + str(profane_value) + '\\t' + line[1] + '\\n')       # wir schreiben alles in die Ausgabedatei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir das Modell trainieren. Wir lesen unsere annotierten Daten mit dem Python-Modul Pandas ein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dataset = pandas.read_csv('training_annotated.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X sind die Werte, die wir auswerten lassen wollen, Y ist die Kategorie, die wir klassifizieren wollen, also OFFENSE oder OTHER. Die Werte sind Zahlen, \"float\", die Kategorie ist ein String. Wir nutzen den \"Decision Tree Classifier\" und speichern das gelernte Modell unter dem Namen \"offense_other_model.sav\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = dataset.values\n",
    "X = array[:,1:4]\n",
    "X = X.astype('float')\n",
    "Y = array[:,4]\n",
    "Y = Y.astype('str')\n",
    "\n",
    "classifier = DecisionTreeClassifier()\n",
    "classifier.fit(X, Y)\n",
    "model_file = 'offense_other_model.sav'\n",
    "pickle.dump(classifier, open(model_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann laden wir das Modell wieder hinein und definieren für einen Satz, dass zunächst die Werte berechnet werden und anschließend das Modell angewendet wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('offense_other_model.sav', 'rb'))\n",
    "\n",
    "def pred_offense_other(sent):\n",
    "    (sentiment_value, offense_value, profane_value) = hate_or_not(sent)\n",
    "    sent_array = [[sentiment_value,offense_value,profane_value]]\n",
    "    result = model.predict(sent_array)[0]\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Ergebnis können wir für einen Satz mit diesem Modell die Kategorie vorhersagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_offense_other(\"Er ist halt ein kriminelles Dreckstück\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Qualität dieses Klassifikationsalgorithmus ist abhängig von der Qualität des Preprocessings, der Sentiment-Analyse und der Wortlisten. Verschiedene Verfahren der Sentiment-Analyse haben wir in den vorhergehenden Kapiteln ausführlich behandelt. Für den Aufbau von Wortlisten gibt es in diesem Kontext die Möglichkeiten, die im Kapitel \"Wörter in der Sentiment-Analyse beschrieben wurden.\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "In diesem Kapitel haben wir die automatische Klassifikation von Aggression in Meinungsäußerungen vorgestellt. Diese Klassifikation ist, auch wenn sie von Menschen und nicht von Algorithmen durchgeführt wird, nicht immer einfach und eindeutig. Im Rahmen der GermEval-Wettbewerbe 2018 und 2019 wurden die Klassen OFFENSE und OTHER mit den Unterklassen INSULT, ABUSE und PROFANITY eingeführt. Die Basis der automatischen Klassifikation sind manuell klassifizierte Daten. Wir haben verschiedene Methoden der Forschungsliteratur vorgestellt und anschließend einen einfachen Algorithmus für die binäre Klassifikation entwickelt. Hier können Sie mit weiteren Arbeiten ansetzen und den vorgestellten Algorithmus als Baseline betrachten. \n",
    "\n",
    "## Übungen\n",
    "- Prüfen Sie Ihr Wissen:\n",
    "    - Welche Arten von aggressiven Äußerungen gibt es?\n",
    "    - Warum sind annotierte Textkorpora wichtig für die Klassifikation?\n",
    "    - Welche Methoden der Klassifikation gibt es?\n",
    "- Setzen Sie Ihr neues Wissen ein:\n",
    "    - Implementieren Sie eine automatische Klassifikation von Tweets in OFFENSE und OTHER, auf Basis der Trainingsdaten der GermEval 2019. \n",
    "    - Wenden Sie Ihren Klassifikationsalgorithmus (bzw. Ihr Modell) auf die Testdaten an. \n",
    "    - Auf der Webseite der GermEval 2019 finden Sie das Evaluationsscript. Werten Sie damit Ihr System aus. Wie gut hätten Sie im Wettbewerb abgeschnitten? \n",
    "    - Welche Techniken haben diejenigen Gruppen verwendet, die besser abgeschnitten haben? Lesen Sie die Beiträge dieser Gruppen im Konferenzband (Struss et al. 2019).\n",
    "- Reflexion in Gruppenarbeit:\n",
    "    - Diskutieren Sie in der Gruppe, für welche Zwecke eine genaue Erkennung der Arten von aggressiven Meinungsäußerungen notwendig und sinnvoll wäre.\n",
    "    - Diskutieren Sie in der Gruppe konkrete Beispiele von Erfahrungen mit Aggression im Internet, die Sie gemacht haben. Welche Bereiche betreffen diese? Auf welchen Social-Media-Kanälen? \n",
    "    - Welche Herausforderungen für die automatische Erkennung stellen Sie für die konkrete Beispiele fest? Begründen Sie Ihre Einschätzung.   \n",
    "\n",
    "\n",
    "## Weiterführende Literatur \n",
    "\n",
    "Mishra et al. (2019a) geben einen Überblick über aktuelle Methoden der Erkennung von Hassrede. Frühe Methoden zur automatischen Klassifikation basieren auf heuristischen Regeln über die Sprache (TF-IDF-Gewichte für Wörter, Sentiment-Analysen u.a.) in Kombination mit Decision Trees und manuell entwickelten Merkmalen für Support Vector Machines (SVM). Lexikonbasierte Ansätze folgen wie z. B. Gitari et al. (2015). Dabei werden auch Methoden entwickelt, um solche Lexika automatisiert zu generieren, wie Wiegand et al. (2018b). Eine alternative Methode zum Aufbau von Wörterlisten ist eine Kombination aus Character-N-Grams, Token-N-Grams und TF-IDF, wie sie vom Gewinner der Shared Task 2018 (Montani 2018) implementiert wurde. Auf der Basis von Wörtern operieren Systeme mit Methoden der Bag-of-words (BOW), wobei in diesem Fall SVMs mit N-Grammen und Lexika kombiniert werden (z. B. Salminen et al. 2018). Dabei stellten Nobata et al. (2016) fest, dass Methoden mit zeichenbasierten N-Grammen (character n-grams) sehr gute Ergebnisse liefern. \n",
    "\n",
    "Seit Kurzem werden Methoden des Deep Learning auch für die Aufgabe der Klassifikation von offensiver Sprache verwendet, wie z. B. von Mishra et al. (2019b). In einem systematischen Vergleich der Methoden (Aken et al. 2018), aber auch als Ergebnis der verschiedenen Shared Tasks der letzten Jahre, wurde deutlich, dass eine Kombination von neuronalen mit nicht-neuronalen Methoden aktuell zu den besten Ergebnissen führt.\n",
    "\n",
    "Einen umfassenden Überblick über Ansätze zur automatischen Klassifikation deutschsprachiger Twitter-Daten bekommen Sie beim Lesen der Tagungsbände der GermEval 2018 und 2019, Ruppenhofer et al. (2018) und Struss et al. (2019). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
