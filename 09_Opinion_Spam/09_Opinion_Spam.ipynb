{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion Spam\n",
    "\n",
    "Die Meinung der Kunden ist ein wichtiger Geschäftsfaktor geworden. Firmen wollen wissen, was Verbraucher von ihrem Produkt oder ihrer Dienstleistung halten, und versuchen, sich und ihre Produkte schnell an Kundenbedürfnisse anzupassen und die geäußerten Meinungen bestenfalls als Marketinginstrument einzusetzen. Manchmal wird man mittlerweile sogar nach Benutzung einer Toilette gebeten, auf einen lachenden oder weinenden Smiley zu drücken.  Für Firmen kann es existenziell sein, vor einem aufkommenden \"Shitstorm\" rechtzeitig gewarnt zu werden. Gleichzeitig kann nur derjenige schnell auf Trends reagieren, der diese auch schnell erkennt.\n",
    "\n",
    "Mit der Zunahme der Relevanz der Kundenmeinungen steigt jedoch auch die Anzahl der Manipulationsversuche. Systematische Untersuchungen zum Anteil der Fake-Bewertungen an den gesamten Bewertungen gibt es bisher nicht, man muss wohl auch davon ausgehen, dass die Anteile je nach Branche sehr unterschiedlich sind. Die Betreiber von Portalen werden diese Zahlen wohl auch nicht veröffentlichen, sofern sie sie haben. Schätzungen sprechen von 20-30%, z. B. Mukherjee (2015). Diese Schätzungen betreffen aber englischsprachige Meinungsäußerungen bezogen auf eine begrenze Anzahl von untersuchten Portalen. In einer Befragung von Hoteliers, durchgeführt an der FH Worms, haben fast die Hälfte der Hoteliers angegeben, Erfahrungen mit gefälschten Bewertungen zu haben. Selbst Erpressungen durch Gäste wurden von Hotelbesitzern berichtet, wobei die Gäste einen Preisnachlass forderten und ansonsten mit negativen Bewertungen drohten  (Conrady 2015).\n",
    "\n",
    " Für die Kunden bedeutet das, dass sie sich nicht immer auf Online-Bewertungen verlassen können und erheblich getäuscht werden. Bewertungen sind Bestandteil des Geschäftsmodells vieler Portale. \n",
    "Da ihre Glaubwürdigkeit erheblich unter den Manipulationen leidet, gehen die Betreiber der Online-Portale mittlerweile gegen Opinion Spam vor, wie z. B. Amazon (http://www.heise.de/newsticker/meldung/Amazon-verklagt-Haendler-wegen-eingekaufter-Fake-Bewertungen-3227189.html). \n",
    "Fake-Bewertungen haben sich zu einem eigenen Geschäftsmodell entwickelt. Es gibt Plattformen und Anbieter für gekaufte Reviews, die ständig öffnen und wieder schließen, wie noch bis Frühjahr 2016 buyamazonreviews.com, fiverr.com  oder reselleratings.com. Inzwischen wird man bei diesen Seiten umgeleitet auf eine Seite von Amazon (https://www.amazon.com/gp/help/customer/display.html?nodeId=201749630&ref=cm_udrp_bar), die dem Nutzer droht:\n",
    "*\"If we determine that you have attempted to manipulate reviews or violated our guidelines in any other manner, we may immediately suspend or terminate your Amazon privileges, remove reviews, and delist related products.\"* (dt.: *Wenn wir feststellen, dass Sie versucht haben, Bewertungen zu manipulieren oder auf andere Weise gegen unsere Richtlinien verstoßen haben, können wir Ihre Amazon-Privilegien sofort aussetzen oder beenden, Bewertungen entfernen und verwandte Produkte aus der Liste nehmen.* (eigene Übersetzung))\n",
    "\n",
    "Die Zeitschrift Ökotest berichtete in ihrer Online-Ausgabe vom 25.Januar 2013 von kriminellen Betrügern, die in einem Online-Portal ein Produkt angeboten, positiv bewertet, dann verkauft und nie ausgeliefert haben. (http://www.oekotest.de/cgi/index.cgi?artnr=11617;gartnr=91;bernr=23) Opinion Spam war hier essenzieller Aspekt der kriminellen Machenschaften.\n",
    "\n",
    "Wir haben es hier also mit einem gesellschaftlich und ökonomisch erheblichen Problem zu tun. Bewertungsportale wie HolidayCheck betreiben einen erheblichen Aufwand, um gefälschte Bewertungen zu löschen (https://www.holidaycheck.de/glaubwuerdigkeit-hotelbewertungen). \n",
    "Es stellt sich daher die Frage, wie Opinion Spam erkannt werden kann und ob es möglich ist, den Erkennungsprozess durch automatische Methoden zu unterstützen. \n",
    "\n",
    "## Gefälschte Bewertungen\n",
    "\n",
    "Im ersten Schritt sehen wir uns an, worum es bei diesem Phänomen eigentlich geht. \n",
    "\n",
    "Liu (2012) unterscheidet zunächst Hype-Spam, bei dem ein Produkt oder eine Firma hochgelobt wird, von Defaming Spam, bei dem verunglimpft wird. Außerdem unterscheidet er \"Fake Reviews\", bei denen es um einzelne Produkte oder Dienstleistungen geht, von unspezifischen Bewertungen von ganzen Firmen oder Dienstleistern (\"Ich liebe XYZ!). \n",
    "\n",
    "Wer sind nun die Personen, die gefälschte Bewertungen verfassen? Erwähnt wurden schon fragwürdige Dienstleister  oder Unternehmen mit kriminellem Hintergrund. Diese Fälle sind eindeutig. Unklarer ist jedoch die Einschätzung, wenn Freunde und Familie eines Buchautors dessen neues Buch bei Amazon bewerten oder die Bewertung eines neuen Autos durch die Mitarbeiter des Herstellers. Was ist darüber hinaus davon zu halten, wenn eine Firma einen Preis dafür ausschreibt, dass Kunden Bewertungen schreiben? \n",
    "Es wird deutlich, dass es hier keine klaren Grenzen gibt und sich die Erkennung von Opinion Spam daher zunächst auf die klaren Fälle beschränken muss.\n",
    "\n",
    "Liu (2012) stellt -- wie im Kapitel 2 beschrieben -- Quintupel auf, um Bewertungen zu beschreiben. Diese Quintupel enthalten die relevanten Elemente einer Bewertung: Die Entität, die bewertet wird, der bewertete Aspekt dieser Entität, die Meinung dazu, der Reviewer und der Zeitpunkt des Reviews. Auf diese Informationen wird auch zugegriffen, wenn man versucht, Fälschungen zu entdecken. Liu (2012) beschreibt die Daten, die für die Entdeckung von Fälschungen zur Verfügung stehen, auf drei Ebenen: Textebene, Meta-Daten und Produktinformationen.\n",
    "Auf der Textebene gibt es Informationen über die Inhalte (wenn Bewertungen unterschiedlicher Produkte sehr ähnlich sind oder etwa die Bewertung eines Buchs einfach aus dem Klappentext kopiert wurde), über die Professionalität (Satzlänge, Fehleranteile), über die benutzten Wörter und über syntaktische und semantische Hinweise auf Lügen.\n",
    "Auf der Ebene der Meta-Daten gibt es Information über die vergebenen Sterne (z. B. die 5-Sterne-Bewertung von Amazon), über die User-ID in einem Bewertungsportal, über den Zeitpunkt des Postings, IP- und MAC-Adressen, Ort des Computers, von dem die Bewertung kommt, und die Reihenfolge von Klicks.\n",
    "Auf der Produktebene haben wir Informationen über den Verkaufsrang und die Produkteigenschaften. \n",
    "\n",
    "## Annotierte Korpora für Opinion Spam\n",
    "\n",
    "Ein wesentliches Problem für die automatische Erkennung  von Opinion Spam: Es steht kein solide validiertes Korpus von gefälschten Bewertungen für die deutsche Sprache zur Verfügung, mit dem man z. B. Data Mining-Systeme trainieren oder testen könnte. Das erste Korpus für das Englische wurde 2011 entwickelt und in Ott et al. (2011) beschrieben. Es enthält 400 echte und 400 gefälschte Bewertungen. Ott et al. (2011) sind so vorgegangen, dass sie positive Hotel-Bewertungen mit fünf Sternen für die 20 populärsten Hotels in der Gegend von Chicago als echte Bewertungen aus TripAdvisor genommen und mit Amazon Mechanical Turk  Versuchspersonen beauftragt haben, gefälschte Bewertungen für diese Hotels zu erstellen. \n",
    "\n",
    "Wang et al. (2012) erstellten ein Korpus für das Englische, indem sie Testpersonen aufforderten, gefälschte Bewertungen für Produkte, die sie nicht kennen, zu produzieren. Das Problem bei dieser Vorgehensweise mit Versuchspersonen ist, dass man nicht sicher sein kann, ob die Ergebnisse authentisch sind. \n",
    " \n",
    "Li et al. (2015) berichten über ein Korpus für das Chinesische mit 6 Millionen Bewertungen für Restaurants in Shanghai, welches auch Metadaten, z. B. Zeit des Postings, IP-Adressen und Orte der Computer enthält. Die chinesische Firma Dianping benutzt einen eigenen Filter für gefälschte Bewertungen, dessen Algorithmus jedoch Firmengeheimnis ist. Die Information darüber, welche Bewertungen gefiltert worden sind, stand den Wissenschaftlern jedoch zur Verfügung. Sie klassifizierten Reviewer (und ihre IP-Adressen), von deren Bewertungen mehr als 50% von Dianping als gefälscht eingestuft werden, als nicht vertrauenswürdige Reviewer.\n",
    " \n",
    "Sandulescu und Ester (2015) hatten ebenfalls Zugriff auf Bewertungen, die von Portalen automatisch gefiltert wurden und damit auf ein Korpus von Bewertungen mit Spam-Verdacht. Unklar sind jedoch Precision und Recall der Filtermethoden, die zu diesem Korpus geführt haben. Das Portal Yelp stellt die gefilterten Beiträge auf den Bewertungsseiten zur Verfügung.  Es ist jedoch auf den ersten Blick nicht ersichtlich, auf welcher Grundlage diese Beiträge gefiltert worden sind. Sehr wahrscheinlich basieren diese Filter eher auf einer Kategorisierung des Verhaltens der Reviewer als auf dem Text der Bewertung. Als Gold-Standard für eine Evaluation oder für Machine Learning-Verfahren eignen sich die deutschen Beispiele jedenfalls nicht. (Z.B. https://www.yelp.de/biz/die-kaffee-d\\%C3\\%BCsseldorf)\n",
    " \n",
    "Shojaee et al. (2015) erstellten ein Korpus, indem sie Annotatoren nach gefälschten Bewertungen in einem Online-Portal suchen lassen und dann das Inter-Annotator Agreement (also die Übereinstimmung zwischen den Annotatoren) gemessen haben. Dies ist sicher eine recht zuverlässige Methode, auch wenn Fake-Bewertungen nicht leicht zu erkennen sind. Die Autoren unterstützten den Annotationsprozess dadurch, dass sie den Annotatoren gezielte Fragen stellten (Z.B.: \"Is this review unrelated to the product?\")  und alle Bewertungen eines Reviewers gleichzeitig präsentierten.\n",
    "\n",
    "Mit Studierenden der Hochschule Darmstadt haben wir Beispiele für Opinion Spam im deutschsprachigen Amazon-Portal gesucht und diese dann analysiert. Dafür haben wir zunächst Kriterien für die Klassifikation der Bewertungen als gefälscht aufgestellt. Sehr häufig verwendeten Reviewer identische Texte für unterschiedliche Produkte. \n",
    "Dabei waren die Texte nichtssagend, also allgemein verwendbar. Auch das Datum ist ein wichtiges Kriterium: Wenn ein Reviewer eine große Anzahl an z. B. Baugeräten einer Firma an einem Tag positiv und die Baugeräte einer anderen Firma am selben Tag negativ bewertet, ist das ein starker Hinweis darauf, dass es sich um eine Fälschung handeln kann. So haben vier Studierende gefälschte Bewertungen im Amazon-Portal gefunden und die Kriterien für die Klassifikation im Textkorpus mit angegeben, sodass die Entscheidung nachvollziehbar ist.\n",
    "\n",
    "Das Annotationsschema für das Korpus deutscher Opinion Spam beinhaltet die Ebenen, die auch Liu (2012) beschrieben hat: Textebene, Meta-Daten und Produktinformationen. \n",
    "\n",
    "- Textebene\n",
    "    - Überschrift\n",
    "    - Text\n",
    "    - Datum\n",
    "    - Tonalität\n",
    "\n",
    "- Meta-Daten\n",
    "    - Review-URL\n",
    "    - Rating\n",
    "        - Anzahl der Bewertungen\n",
    "        - Anzahl der Bewertungen mit 5 Sternen\n",
    "        - Anzahl der Bewertungen mit 1 Stern\n",
    "        - diese Bewertung\n",
    "\n",
    "- Reviewer\n",
    "    - User-ID\n",
    "    - verifizierter Kauf\n",
    "    - Bewertung des Reviews als hilfreich\n",
    "\n",
    "- Produktinformation\n",
    "    - Produktname\n",
    "    - Verkaufsrang\n",
    "\n",
    "- Shop\n",
    "\n",
    "- Begründung für die Klassifikation\n",
    "\n",
    "\n",
    "Das so entstandene Textkorpus mit 218 als gefälscht klassifizierten und 60 als echt klassifizierten Bewertungen ist auf der Webseite zu diesem Buch verfügbar. Es ist für maschinelles Lernen noch zu klein, aber schon groß genug, um einige statistische Untersuchungen zu machen und heuristische Methoden für eine Klassifikation und daraus Features für Machine-Learning-Methoden zu entwickeln.\n",
    "\n",
    "## Klassifikation von Bewertungen\n",
    "Die Erkennung von Opinion Spam ist eine klassische Klassifikationsaufgabe, die Dokumente (Bewertungen) als gefälscht oder als nicht gefälscht klassifizieren soll. Im Folgenden werden Methoden für diese Klassifikationsaufgabe vorgestellt. Diese Methoden unterscheiden sich vor allem dadurch, auf welche Daten sie sich beziehen.\n",
    "\n",
    "## Klassifikation mit Meta-Daten\n",
    "Hooi et al. (2016) klassifizieren Reviewer auf der Basis ihres Verhaltens. So sind beispielsweise Reviewer, die ausschließlich positive Bewertungen in großer Menge abgeben, verdächtig, ebenso wie Reviewer, die viele Bewertungen in einer sehr kurzen Zeit abgeben. Mit diesen Meta-Daten trainieren sie ein Bayesianisches Modell des Data Mining, um verdächtige Reviewer zu finden.\n",
    "\n",
    "Wang et al. (2012) betrachten die Relation zwischen dem Reviewer, den Bewertungen und dem Shop, der das Produkt anbietet. Sie teilen also die Klassifikationsaufgabe in drei Unteraufgaben ein. Sie stellen fest, dass nicht vertrauenswürdige Reviewer (Spammer) eine Beziehung zum Shop haben. Weiterhin stellen sie fest, dass es gute und schlechte Shops gibt und dass schlechte Shops Spammer engagieren. Fake-Bewertunge sind nicht ehrlich. Für die Klassifikation der Bewertungen betrachtet man also die Korrelationen der Vertrauenswürdigkeit der Reviewer, der Echtheit der Bewertungen und der Seriosität des Shops:\n",
    "\n",
    "- Die Vertrauenswürdigkeit des Reviewers ist abhängig von der Anzahl seiner echten Bewertungen.\n",
    "- Ein Shop ist seriös, wenn viele vertrauenswürdige Reviewer ihn positiv bewerten, und weniger serös, wenn viele vertrauenswürdige Reviewer ihn negativ bewerten.\n",
    "- Die Echtheit einer Bewertung hängt von der Seriosität des Shops und der Übereinstimmung mit anderen Bewertungen in einem gegebenen Zeitfenster ab.\n",
    "\n",
    "\n",
    "Als gravierendes Problem für die Evaluation dieser Methode wird auch von Wang et al. (2012) hervorgehoben, dass es kein ausreichend großes Korpus mit Bewertungen gibt, bei denen zuverlässig annotiert ist, ob sie echt oder falsch sind. \n",
    "\n",
    "Mit der Datenbasis, die Li et al. (2015) mit ihrem chinesischem Korpus haben, konnten sie Regelmäßigkeiten bzgl. Zeit und Ort feststellen. So sind Spammer häufiger an Wochentagen aktiv als vertrauenswürdige Reviewer, die eher am Wochenende Restaurants bewerten. Je weiter entfernt die Reviewer von Shanghai waren, desto mehr Spammer waren unter ihnen. \n",
    " Li et al. (2015) zeigen so die Möglichkeit auf, die Klassifikation von Reviewern auf der Basis von vorklassifizierten Daten zu lernen. Verknüpft mit den Methoden von Wang et al. (2012) könnte man mit diesen Ergebnissen Shops und Bewertungen klassifizieren. \n",
    " \n",
    "Die Zeit des Postings von Bewertungen betrachten Ye et al. (2016) unter verschiedenen Aspekten, die sie als Alarmsignale interpretieren, wie:\n",
    "\n",
    "- wenn die durchschnittliche Bewertung eines Produkts sich plötzlich ändert\n",
    "- wenn plötzlich extrem viele Bewertungen zu einem Produkt erscheinen\n",
    "- wenn Reviewer regelmäßig jeden Tag Bewertungen posten\n",
    "\n",
    "Banerjee et al. (2015) weisen darauf hin, dass Fake-Bewertungen oft ohne den vorherigen Kauf eines Produkts oder den Aufenthalt in einem bewerteten Hotel stattfinden. Auch Amazon markiert Bewertungen mit \"verifizierter Kauf\", wenn ein Produkt über Amazon bestellt worden ist. Hier sind also Metadaten vorhanden, die helfen können, Bewertungen als echt zu klassifizieren und ein Korpus aus echten Bewertungen aufzubauen.\n",
    "\n",
    "## Klassifikation mit linguistischer Information\n",
    "\n",
    "Sandulescu und Ester (2015) stellen fest, dass Spammer zwar häufig ihre Benutzernamen wechseln, aber dennoch häufig sehr ähnliche Texte für unterschiedliche Produkte schreiben. Die Autoren berechnen daher die Ähnlichkeit zwischen Bewertungen unter Verwendung von Synonymie-Beziehungen aus WordNet sowie Informationen über Lemmatisierung von Wörtern und ihren syntaktischen Kategorien. Sie bestimmen dann in Experimenten einen Grenzwert, ab dem die Bewertungen anderen so ähnlich sind, dass sie als gefälscht klassifiziert werden können. Als Datenbasis für die Evaluation nehmen sie Bewertungen aus Yelp und Trustpilot, wobei sie die von den Firmen gefilterte Bewertungen als gefälscht klassifizieren. Damit knüpfen  sie an Experimente von Jindal und Liu (2008) an, die ebenfalls nach Bewertungen suchen, die ein hohes Maß an Ähnlichkeit aufweisen und damit ein Korpus von Fake Reviews aufbauen.\n",
    "\n",
    "Banerjee et al. (2015) erstellen ein Korpus gefälschter Bewertungen mithilfe von Versuchspersonen. Dieses Korpus nutzen sie, um linguistische Hinweise in Bewertungen zu sammeln, die auf Fakes hindeuten. Sie unterscheiden dabei vier linguistische Merkmale: Verständlichkeit, Detailgenauigkeit, Schreibstil und Kognitionsindikatoren. Für die Bestimmung der Verständlichkeit wurden Standard-Verständlichkeitsmaße wie der \"Flesch-Kincaid Grade Level\" (Kincaid et al. 1975) berechnet. Detailgenauigkeit wird zunächst mit den verwendeten syntaktischen Kategorien (POS) berechnet. In informativen Texten gibt es mehr Nomen, Adjektive, Artikel und Präpositionen als Verben, Konjunktionen, Adverbien und Pronomen. Dazu kommt die Berechnung der lexikalischen Diversität. Die Berechnung des Schreibstils beruht auf der Benutzung von Emotionswörtern, dem Tempus der Verben, dem Gebrauch von verstärkenden Wörtern wie \"always\" oder \"never\" und Satzzeichen wie Fragezeichen oder Ausrufezeichen. Kognitionsindikatoren sind sprachliche Merkmale, die auf Lügen hindeuten, wie z. B. der Gebrauch von Wörtern wie \"should\" und \"may\" oder auch der Gebrauch von Füllwörtern. Mithilfe dieser Merkmale trainieren sie Machine-Learning-Systeme. \n",
    "Im Ergebnis zeigt sich, dass die linguistischen Merkmale dabei helfen, echte von gefälschten Reviews zu unterscheiden. Kritisch dabei ist jedoch, dass das Korpus der gefälschten Bewertungen in einer künstlichen Experiment-Situation entstanden ist und daher seine Authentizität in Frage gestellt werden kann. \n",
    "\n",
    "## Beobachtungen über Opinion Spam im deutschsprachigen Amazon-Portal\n",
    "\n",
    "Sehen wir uns einmal die Beispiele für Opinion Spam an, die die Studierenden der Hochschule Darmstadt im deutschsprachigen Amazon-Portal gefunden haben. Diese Beispiele sind auf der zum Buch gehörenden Webseite verfügbar.\n",
    "Erste Beobachtungen zeigen, dass die Forschungsergebnisse für das Englische und das Chinesische zum Teil auf das Deutsche übertragbar sind. \n",
    "\n",
    "Wir haben uns auf Hype-Spam und Fake Reviews konzentriert, weil wir vor allem diese im deutschen Amazon-Portal gefunden haben. Wenige Beispiele für Defaming Spam sind aber ebenfalls dabei. Anders als von Wang et al. (2012) beobachtet, scheint im deutschen Amazon-Portal aus dem Jahr 2016 der Shop nicht ausschlaggebend zu sein. Wenn wir eine gefälschte Bewertung gefunden haben und weitere Bewertungen zu Produkten im selben Shop analysiert haben, so haben wir nur sehr selten weitere gefälschte Bewertungen gefunden. Es müsste somit untersucht werden, ob eher die Herstellerfirma (z. B. im Fall von technischen Geräten) oder der Autor, Komponist oder ähnliche Protagonisten Opinion Spam in Auftrag geben. \n",
    "\n",
    "Wie auch Hooi et al. (2016) feststellen, haben wir häufig verdächtige Reviewer gefunden, die denselben Text am selben Datum für verschiedene Produkte verwenden. Dies ist auch ein Ansatzpunkt für eine Erweiterung des Korpus, denn weitere Bewertungen von notorischen Spammern können aufgenommen werden.\n",
    "\n",
    "Das Datum scheint eine Rolle zu spielen, etwa wenn es direkt nach Erscheinen einer CD sehr viele positive Bewertungen innerhalb weniger Tage gibt und später dann in erster Linie negative. Wir können – ebenso wie Li et al. (2015) – feststellen, dass die Spammer meist an Wochentagen und seltener an Wochenenden agieren. Es sind nur ca. ein Viertel der gefälschten Bewertungen am Wochenende entstanden und der Rest an einem Wochentag. Allerdings sind auch ca. 70% der echten Bewertungen an einem Wochentag entstanden. \n",
    "\n",
    "Anders als bei Banerjee et al. (2015) festgestellt, handelt es sich bei den gefälschten Bewertungen im deutschen Amazon-Portal oft um verifizierten Kauf, im Korpus in ca. 82% der Fälle. Dies deutet auf eine gewisse Professionalität der Spammer hin, die entweder direkt von den Shops beauftragt werden oder die Produkte bestellen und danach zurücksenden. Jedenfalls scheint für das deutsche Amazon-Portal des Jahres 2016 die Methode des Korpusaufbaus mit nicht verifizierten Käufen nicht zu funktionieren.\n",
    "\n",
    "Die verdächtigen Texte – gerade wenn sie von Spammern mehrfach verwendet werden – sind wenig konkret, z. B.: \n",
    "\n",
    "\"Alles bestens und schnell wie immer gelaufen - würde ich immer wieder wiederholen. Die Ware ist OK \n",
    " also die lieferung ist schnell und unkompliziert. die ware ist top und es gibt keine beanstandungen. da würde ich wieder bestellen. :-)\"\n",
    " \n",
    " Häufig beziehen sich die Spammer auf die Lieferung, wie im oben genannten Beispiel, und nicht auf das Produkt selbst, da sie dann für jedes Produkt eine eigene Bewertung schreiben müssen. Manche versuchen jedoch, auch diesen Prozess zu automatisieren, was im folgenden Fall schiefgegangen ist, weil die Variablen im Text geblieben sind:\n",
    "\n",
    "\"Ich kann das oben angegebene Produkt \\\\$article\\_name  vorbehaltlos empfehlen. Als ich \\\\$article\\_medium  endlich erwerben konnte, war ich mehr als positiv überrascht. Ich werde auch in Zukunft \\\\$article\\_name  immer wieder konsumieren und habe gleich noch einmal zugegriffen, da auch der Preis \\\\$article\\_price  für das Produkt \\\\$article\\_name  sehr gut ist. Ich freue mich schon auf weitere sehr gute Angebote von \\\\$article\\_manufacturer.\"\n",
    "\n",
    "Die durchschnittliche Anzahl an Nomen ist in echten Bewertungen etwa doppelt so hoch wie in gefälschten. Autoren von Bewertungen nutzen Nomen, um konkrete Informationen über Aspekte des Produkts zu geben, das sie bewerten. Diese Texte können dann nicht mehr für andere Produkte weiterverwendet werden. Viele der häufig in gefälschten Bewertungen genutzten Nomen sind eher unspezifisch, so wie \"Produkt\" oder \"Preis\". Die lexikalische Diversität ist daher in gefälschten Bewertungen geringer als in echten. \n",
    "\n",
    "Gefälschte Texte sind im Durchschnitt kürzer als echte Bewertungen (38 Tokens pro gefälschter Bewertung vs. 71 Tokens pro echter Bewertung). Viele Spammer reagieren auf die Anforderungen von Amazon nach einer Mindestlänge eines Reviews von 20 Wörtern mit Tricks, wie sinnlose Sätze, Wiederholungen und Wörtern mit Leerzeichen zwischen den Buchstaben:\n",
    "\n",
    "\"alles war gut, ich habe leider keine weitere Lust noch mehr dazu zu schreiben mit recht freundlichen grüßen danke !!!\\\\\n",
    "gefällt mir, sieht gut aus, ist sehr praktisch,einfach gut,gefällt mir, sieht gut aus, ist sehr praktisch,einfach gut,gefällt mir, sieht gut aus, ist sehr praktisch,einfach gut\\\\\n",
    "Hab den Anhänger damals für ne Freundin bestellt - hat Ihr gefallen - e m p f e h l e n s w e r t\"\n",
    "\n",
    "\n",
    "Die durchschnittliche Anzahl der Tokens, die nur aus einem Zeichen bestehen, ist in gefälschten Bewertungen deutlich höher.\n",
    "\n",
    "Tabelle 9.1 zeigt die Verteilung der Metadaten-Merkmale im annotierten Textkorpus. Man sieht, dass der Salesrank bei echten Bewertungen signifikant höher ist. Anders als erwartet, haben gefälschte Bewertungen häufiger den Status \"verified\" als echte Bewertungen. \n",
    "\n",
    "|                        | FAKE    | GENUINE|\n",
    "|------------------------|---------|--------|\n",
    "|salesrank               | 142,660 | 418,591|\n",
    "|no\\_ratings             | 83.51   | 82.10  |\n",
    "|thisrating              | 4.60    |  3.32  |\n",
    "|no\\_five\\_star\\_ratings | 51.98   | 48.85  |\n",
    "|no\\_one\\_star\\_ratings  | 9.44    | 13.70  |\n",
    "|verified                | 0.82    |  0.75  |\n",
    "|review\\_date\\_weekend   | 0.24    |  0.30  |\n",
    "Tabelle 9.1: Durchschnittliche Werte für Metadaten-Merkmale in gefälschten und echten Bewertungen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabelle 9.2 zeigt die Verteilung der linguistischen Merkmale im annotierten Textkorpus. Gefälschte Bewertungen sind tendenziell kürzer: Die Anzahl der Tokens sowohl in den Überschriften als auch im Text ist bei echten Bewertungen signifikant höher. Die Anzahl der Tokens, die nur aus einem Zeichen bestehen, ist bei gefälschten Bewertungen höher. \n",
    "\n",
    "\n",
    "\n",
    "|                            | FAKE    | GENUINE|\n",
    "|----------------------------|---------|--------|\n",
    "|no\\_tokens\\_headline        |  2.63   |  3.90  |\n",
    "|no\\_tokens\\_text            | 37.56   | 70.82  |\n",
    "|one\\_letter\\_tokens         |  0.21   |  0.17  |\n",
    "|no\\_nouns                   |  0.17   |  0.2   |\n",
    "|no\\_adjectives              |  0.1    |  0.1   |\n",
    "|no\\_verbs                   |  0.13   |  0.12  |\n",
    "|no\\_selfreferring\\_pronouns |  1      |  1.05  |\n",
    "Tabelle 9.2: Durchschnittliche Werte für linguistische Merkmale in gefälschten und echten Bewertungen}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maschinelles Lernen für die automatische Klassifikation\n",
    "\n",
    "Eine automatische Klassifikation als gefälschte Bewertung erscheint mit maschinellen Lernverfahren also insgesamt als möglich. Wir kennen die  Merkmale von Bewertungen, die für die Klassifikation relevant sein können. Mit diesen Merkmalen trainieren wir jetzt auf unseren Daten ein Modell. \n",
    "Jede der Bewertungen im Korpus wird als Vektor mit folgenden Informationen dargestellt:\n",
    "\n",
    "- salesrank (Zahlenwert)\n",
    "- number\\_of\\_ratings\\_for\\_this\\_product (Zahlenwert)\n",
    "- this\\_rating (Zahlenwert 1-5)\n",
    "- number\\_of\\_five\\_star\\_ratings\\_for\\_this\\_product (Zahlenwert)\n",
    "- number\\_of\\_one\\_star\\_ratings\\_for\\_this\\_product  (Zahlenwert)\n",
    "- verified (1 oder 0)\n",
    "- review\\_date\\_weekend  (1 oder 0)\n",
    "- number\\_of\\_tokens\\_headline (Zahlenwert)\n",
    "- number\\_of\\_tokens\\_text (Zahlenwert)\n",
    "- selfreferring\\_pronouns (Prozentanteil aller Tokens)\n",
    "- nouns (Prozentanteil aller Tokens)\n",
    "- adjectives (Prozentanteil aller Tokens)\n",
    "- verbs (Prozentanteil aller Tokens)\n",
    "- one\\_letter\\_tokens (Prozentanteil aller Tokens)\n",
    "- fake\\_words (Zahlenwert)\n",
    "- class (echt oder gefälscht)\n",
    " \n",
    " Die Meta-Daten (salesrank, number\\_of\\_ratings\\_for\\_this\\_product, this\\_rating, number\\_of\\_five\\_star\\_ratings\\_for\\_this\\_product, number\\_of\\_one\\_star\\_ratings\\_for\\_this\\_product) können dabei direkt aus dem XML extrahiert werden, während die linguistischen Daten mithilfe einer linguistischen Analyse, z. B. mit TextBlob oder spaCy, erfasst werden. Um eine Liste von \"Fake Words\" aufzustellen, also Wörtern, die vor allem in gefälschten Bewertungen vorkommen, machen wir eine Differenzanalyse wie auch schon bei den Sentimentwörtern: Wir vergleichen die Wörter in den gefälschten Bewertungen mit denen in den echten Bewertungen und nehmen in die Liste diejenigen Wörter auf, die nur in gefälschten Bewertungen vorkommen. Diese Liste wird dann manuell nachbearbeitet. \n",
    " \n",
    "Reviews sind dann z. B. so dargestellt:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " 2919,73,5,71,2,1,0,1,20,0,0.25,0.0,0.1,0,0.05,FAKE\n",
    " 21,514,5,331,64,1,0,2,27,0,0.37,0.07,0.04,0,0.0,GENUINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Vektoren können nun als Trainingsmaterial für maschinelles Lernen verwendet werden. Wie im Kapitel 3.6.2 beschrieben, arbeiten wir hier mit Pandas für die Datenanalyse und Sklearn für das maschinelle Lernen.\n",
    "\n",
    "\n",
    "\n",
    "## Zusammenfassung\n",
    "\n",
    "Sentiment-Analyse verliert ihren Wert in dem Maße, in dem gefälschte Bewertungen - Opinion Spam - in den Foren auftritt. Die Nutzer wie die Portale leiden erheblich unter dem Verlust an Glaubwürdigkeit der Bewertungen. \n",
    "Daher müssen Algorithmen entwickelt werden, die diese Fälschungen entdecken und klassifizieren. Wir haben zunächst definiert, welche Arten vorn Fälschungen es gibt. Da es für eine automatische Klassifikation unablässig ist, auf annotierte Textkorpora zuzugreifen, haben wir im nächsten Schritt annotierte Korpora dargestellt und sind näher auf ein kleines Textkorpus deutschsprachiger Opinion Spam eingegangen. Anhand der Forschungsliteratur haben wir verschiedene Klassifikationsmethoden anhand von Meta-Daten und anhand von linguistischer Information vorgestellt. Mit einer Analyse des annotierten Textkorpus haben wir Merkmale identifiziert, die für eine Klassifikation relevant sind. Mit diesen Merkmalen können wir anschließend mit maschinellem Lernen ein Modell trainieren. \n",
    "\n",
    "## Übungen\n",
    "1. Prüfen Sie Ihr Wissen:\n",
    "- Welche Informationen stehen für die Erkennung von Opinion Spam zur Verfügung?\n",
    "- Warum ist das Fehlen eines zuverlässigen annotierten Textkorpus für Opinion Spam ein Problem?\n",
    "- Welche Methoden zur Erkennung gibt es?\n",
    "\n",
    "\n",
    "2. Setzen Sie Ihr neues Wissen ein:\n",
    "- Suchen Sie im Amazon-Portal  manuell nach gefälschten Bewertungen und verifizieren Sie dabei die im Text genannten Kriterien. \n",
    "- Nehmen Sie den annotierten Textkorpus von der Buch-Website zur Hand und konvertieren Sie die Bewertungen in Vektoren, wie im Text beschrieben. Trainieren Sie anschließend ein Modell mit Methoden des maschinellen Lernens darauf. \n",
    "- Evaluieren Sie dieses Modell an den von Ihnen selbst gesammelten Bewertungen.\n",
    "\n",
    "\n",
    "3. Reflexion in Gruppenarbeit:\n",
    "- Diskutieren Sie in der Gruppe, für wen Opinion Spam ein Problem sein kann (Öffentlichkeit/Online-Nutzer, Portal-Betreiber, Unternehmen). \n",
    "- Diskutieren Sie in der Gruppe, ob manipulierte Bewertungen als ein \"legitimes\" Instrument der Verkaufsförderung ist. Betrachten Sie in diesem Zusammenhang die Möglichkeit von \"incentivized\" Bewertungen, also den Einsatz von Anreizsystemen zur Review-Abgabe, z. B. Bonus-Meilen, monetäre Vergütung, Urlaubsreise, kostenfreie Produkte.\n",
    "\n",
    "\n",
    "## Weiterführende Literatur\n",
    "\n",
    "Einen aktuellen Überblick über Forschungsmethoden geben Ren und Li (2019). Dort werden auch Arbeiten vorgestellt, die  neuronale Netze nutzen.\n",
    "Das Problem der Korpuserstellung wird von Ott et al. (2011), Wang et al. (2012), Li et al. (2015), Sandulescu und Ester (2015) und Shojaee et al. (2015) adressiert.\n",
    "Beispiele für Klassifikationen mit Meta-Daten sind in Hooi et al. (2016), Wang et al. (2012) und Li et al. (2015) zu finden. Eine Klassifikation mit linguistischen Merkmalen beschreiben Sandulescu und Ester (2015) und Banerjee et al. (2015).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
